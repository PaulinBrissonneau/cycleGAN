\chapter{Multi-Layer Perceptron}

\section{Les neurones}

Les technologies qui vont êtres présentées dans la suite se basent outes sur l'idée de neurone artificiel. McCulloh et Pitts le formalise en 1943 \cite{McCullohPitts}.
Un neurone formel est composée de deux parties.\begin{itemize}
\item La première consiste à faire la sommes pondérée par des poids des valeurs d'entrée du neurone auquel on peut éventuellement ajouter un biais. Les poids sont propres au neurone et à l'entrée du neurone considérée.
\item La deuxième partie du neurone est la fonction d'activation. Cette fonction va s'appliquer sur le résultat de la somme pondérée. On choisit quasi-exclusivement des fonctions non-linéaires pour deux raisons : briser la linéarité (car sinon la fonction n'apporte rien de plus que les poids) et obtenir un résultat d'une certaine forme (par exemple une probabilité entre 0 et 1).
McCulloh et Pitts dans leur première ébauche du neurone formel considérèrent des neurones au résultat binaire à l'aide d'une fonction de'activation de Heavyside.

\end{itemize}


\begin{figure}[!h]
\centering
\includegraphics[width=150pt,valign=t]{"images/MLP/neurone_exemple"}
\caption{Exemple de neurone dont les poids sont $w_1$ et $w_2$ la fonction de Heavyside de paramètre $\theta$ s'applique à la somme pour donner la sortie du neurone. Cette image provient de l'article wikipédia du neurone formel.}
\label{neurone_exemple}
\end{figure}

Ainsi formellement en utilisant les notant $X = \begin{pmatrix} x_1\\ \vdots \\ x_n \end{pmatrix}$  les entrées du neurone, $W = \begin{pmatrix} w_1\\ \vdots \\ w_n \end{pmatrix}$ les poids correspondants et $\phi$ la fonction d'activation, $b$ le biais, un neurone correspond à une fonction $N(x_1,...,x_n,w_1,...,w_n) = \phi(b+ \sum\limits_{i} x_i w_i) = \phi(W^T  X+b)$



\section{Le perceptron}
En 1958, Frank Rosenblatt utilise l'idée de neurones artificiels pour inventer le perceptron\cite{}. Son idée est d'utiliser les neurones pour reconnaître des patterns. Cependant dans cette forme simple, il conserve les neurones tels que définis en 1943 avec la fonction de Heavyside et une seule couche de neurones (ie les neurones ne sont pas reliés entre eux). Cela limite l'intérêt de ce perceptron : il ne peut apprendre que des pattern linéairement séparables. Minsky démontre par exemple que le perceptron est incapable d'effectuer un XOR.
L'idée est d'ajuster les poids pour que la sortie du neurone soit 1 si et seulement si l'entrée est dans l'ensemble que l'on cherche à reconnaître. 

\begin{figure}[!h]
\centering
\includegraphics[width=150pt,valign=t]{"images/MLP/perceptron"}
\caption{Exemple de perceptron à n entrée et p sorties. Il peut donc servir de classificateur à p classes.}
\label{perceptron}
\end{figure}

\section{Multi-Layer Perceptron}
Un Multi-Layer Perceptron (MLP) est composé de différentes couches: \begin{itemize}
\item La couche d'entrée qui correspond aux valeurs d'entrée de l'algorithme
\item une couche de sortie qui correspond aux valeurs renvoyées par le MLP
\item une ou plusieurs couches "cachées" (hidden layers) qui sont des couches de neurones reliées entre elles. Les sorties de la couche précédente servent d'entrée à la couche suivante.

\end{itemize}

Ecrivons formellement ceci. On va définir $N^{(i)}  = \begin{pmatrix} n_1^(i)\\ \vdots \\ n_{p_i}^{(i)} \end{pmatrix}$  et 
$W_k^{(i)} = \begin{pmatrix} w_{1k}^{(i)}\\ \vdots \\ w_{p_ik}^{(i)} \end{pmatrix}$ les poids associés au $k^e$ neurone de la couche i. la  et $\phi_k^{(i)}$ la fonction d'activation de ce même neurone. 
On notera que si le réseau possède $L+1$ couches (de $0$ à$ L$), $N^{(0)} = X$ et $N^{(L)} = Y$

Dès lors, $\forall i \in [1,L], N^{(i)}_k =  $





\subsection{Quelques fonctions d'activations}
Il existe de nombreuses fonctions d'activation avec chacune leurs intérêts et défauts. En voici quelques-unes.
\subsubsection{Sigmoïde}
 Cette fonction permet d'avoir des valeurs de sortie entre 0 et 1mais souffre d'un problème de gradient vanishing (cf plus loin). Elle introduit un hyperparamètre $\lambda$.
\begin{displaymath}
\varphi_\lambda (x) = \frac{1}{1+e^{-\lambda x}}
\end{displaymath}

\begin{figure}[!h]
\centering
\includegraphics[width=150pt]{"images/MLP/Sigmoide"}
\caption{Fonction sigmoïde(Wikipedia)\\}
\label{sigmoide}
\end{figure}


\subsubsection{Tangente hyperbolique}
Cette fonction permet d'avoir des valeurs de sortie entre -1 et mais souffre d'un problème de gradient vanishing (cf plus loin)
\begin{displaymath}
\varphi (x) = tanh(x)
\end{displaymath}

\begin{figure}[!h]
\centering
\includegraphics[width=150pt]{"images/MLP/tanh"}
\caption{Fonction tangente hyperbolique(Wikipedia)\\ }
\label{tanh}
\end{figure}


\subsubsection{Softmax}
Cette fonction s'applique sur un vecteur, elle est souvent utilisée en sortie car elle permet d'obtenir des données interprétables comme des probabilités(entre 0 et 1 et dont la somme vaut 1)
\begin{displaymath}
\varphi_\lambda (X)_j = \frac{e^{-x_j}}{\sum\limits_{k}  e^{-x_k}}
\end{displaymath}

\subsubsection{ReLu}
Permet d'éviter le gradient vanishing, accélère les calculs mais peut mener à la mort des neurones(cf dropout).
\begin{displaymath}
\varphi (x) = max(0,x)
\end{displaymath}

\begin{figure}[!h]
\centering
\includegraphics[width=150pt]{"images/MLP/ReLu"}
\caption{Fonction reLu(Wikipedia)}
\label{ReLu}
\end{figure}

\subsubsection{Et bien d'autres...}
Il existe bien d'autres fonctions d'activation ayant divers effets. Ce champ de recherche est d'ailleurs actif : de nouvelles fonctions sont découvertes chaque année et dont l'effet n'est pas toujours bien compris. On peut par exemple citer la Mish(2019)\cite{Mish} ou la softexponential(2016)\cite{Softexponential}
\begin{figure}[!h]
\centering
\includegraphics[width=150pt]{"images/MLP/Mish"}
\caption{Fonction Mish}
\label{Mish}
\end{figure}


\section{Implémentation et résultats}
\subsection{Avant-propos}
Nous avons étudié l'impact de différents hyperparamètres sur les résultats du MLP. Sauf mention contraire, le jeu de donnée utilisée est MNIST. MNIST est un ensemble d'images en niveau de gris de chiffres écrits à la main. Le MLP est alors utilisé en tant que classificateur. Il doit reconnaître à partir de l'entrée du niveau de gris des pixels quel est le chiffre dessiné. Les résultats de précision sont calculés sur un jeu de données de test composé de données choisies aléatoirement avant l’entraînement et n'étant pas utilisées pour celui-ci. Afin de mesurer l'effet des hyperparamètres, on trace toujours les courbes avec tous les hyperparamètres égaux sauf celui qui varie.
Les MLP utilisés pour les mesures ont été réalisés from scratch. La configuration à partir de laquelle on fait varier les hyperparamètres à été obtenu par ajustement à la main et ne prétend pas être optimale. Toutefois, les valeurs observées été de l'ordre de celles trouvées dans la littérature. Les données sont moyennées sur un grand nombre de lancers de l'algorithme et l'intervalle de confiance à 95\% est indiqué.

\begin{figure}[!h]
\centering
\includegraphics[width=150pt]{"images/MLP/MNIST_exemple"}
\caption{Exemple de résultat de classification avec le MLP\\ }
\label{MNIST_exemple}
\end{figure}

\subsection{Effet du nombre de neurone par couche}
On s'intéresse ici à la largeur du réseau i.e. le nombre de neurones par couches.

\begin{figure}[!h]
\centering
\includegraphics[width=150pt]{"images/MLP/MLP_largeur"}
\caption{Courbes de précision sur les données de test en fonction de l'avancement de l'apprentissage. On observe que si trop peu de neurones ne permet pas une classification satisfaisante,  trop de neurones est contre productif. Il s'agit alors de trouver le juste milieu.\\ }
\label{MLP_largeur}
\end{figure}

\subsection{Effet du nombre de couches de neurones}
On s'intéresse ici à la profondeur du réseau.
\begin{figure}[!h]
\centering
\includegraphics[width=150pt]{"images/MLP/MLP_profondeur"}
\caption{Courbes de précision sur les données de test en fonction de l'avancement de l'apprentissage. Une seule couche correspond donc à un perceptron simple. On observe que le MLP est plus performant. Cependant augmenter la profondeur du réseau réseau réduit sa vitesse de convergence.\\ }
\label{MLP_profondeur}
\end{figure}

\subsection{Taille des batchs}

\section{Rétropropagation}


\subsection{Algorithmes d'optimisation}
La fonction de coût utilisée dans toute cette partie est $J = \sum\limits_{k} (y_{th} - y_{mes})^2)$


\subsubsection{Descente de gradient stochastique}
Il s'agit de l'algorithme classique.  $w_{jk}^{(i)} \leftarrow w_{jk}^{(i)} - \alpha \frac{\partial J}{\partial w_{jk}^{(i)}}$ . Un coefficient $\alpha$ appelé taux d'apprentissage défini l'ampleur de la modification du poids. Plus $\alpha$ est grand, plus le pas est important. Ainsi un taux d'apprentissage trop faible entraînera une convergence trop lente ou un blocage dans un minimum local mais un taux trop élevé risque de ne jamais converger. 

\begin{figure}[!h]
\centering
\includegraphics[width=130pt]{"images/MLP/descentedegradientstochastique"}
\hspace*{10mm}
\includegraphics[width=130pt]{"images/MLP/descentedegradientstochastiquepropre"}
\caption{Exemple d'application de la descente de gradient stochastique. La figure représente le cheminement de la valeur des deux poids. La hauteur représente l'erreur. On voit bien ici que l'algorithme peut entraîner un blocage dans un minimum local.}
\label{descentedegradientstochastique}
\end{figure}



\subsubsection{Descente de gradient newtonienne}
Ici, on ajoute un terme d'inertie à la modification du poids cela entraîne l'apparition d'un nouvel hyperparamètre $\beta_1$ qui sert à doser cette inertie. On note $\nu$ le terme de descente au pas précédent. $ w_jk^{(i)} \leftarrow w_jk^{(i)} + \beta_1 \nu - \frac{\partial J}{\partial w_{jk}^{(i)}}$
	
\begin{figure}[!h]
\centering
\includegraphics[width=130pt]{"images/MLP/descentedegradientnewtonnienne"}
\hspace*{10mm}
\includegraphics[width=130pt]{"images/MLP/descentedegradientnewtonniennepropre"}
\caption{Exemple d'application de la descente de gradient stochastique. La figure représente le cheminement de la valeur des deux poids. La hauteur représente l'erreur. On remarque que l'algorithme est bien plus instable mais explore plus et ne reste pas forcément bloqué dans les minimum locaux.}
\label{descentedegradientstochastique}
\end{figure}

\subsubsection{RMSProp}
L'algorithme est le suivant : on choisit les hyperparamètre $\beta_2$ $\alpha$ et $\epsilon$ tel que epsilon soit petit mais non nul, on initialise $\nu$ à 0.
A chaque pas on calcule :

\begin{displaymath}
s \leftarrow \beta_2 s + (1-\beta_2)  (\frac{\partial J}{\partial w_{jk}^{(i)}})^2 \\
w_{jk}^{(i)} \leftarrow w_{jk}^{(i)} - \frac{\alpha}{\sqrt{s+\epsilon}}\frac{\partial J}{\partial w_{jk}^{(i)}}
\end{displaymath}

\begin{figure}[!h]
\centering
\includegraphics[width=130pt]{"images/MLP/RMSProp"}
\hspace*{10mm}
\includegraphics[width=130pt]{"images/MLP/RMSPropPropre"}
\caption{Exemple d'application de RMSProp. La figure représente le cheminement de la valeur des deux poids. La hauteur représente l'erreur. L'algorithme est sensible au minimum locaux mais peu aux grands gradients.}
\label{RMSProp}
\end{figure}

\subsubsection{ADAM}
Il s'agit d'une combinaison de RMSProp et de la descente de gradient newtonnienne.
L'algorithme est le suivant : on choisit les hyperparamètre $\beta_1$ $\beta_2$  $\alpha$ et $\epsilon$ tel que epsilon soit petit mais non nul, on initialise $\nu$ et $s$ à 0.
A chaque pas on calcule :
\begin{displaymath}
\nu \leftarrow \beta_1 \nu + (1-\beta_1) \frac{\partial J}{\partial w_{jk}^{(i)}} \\
s \leftarrow \beta_2 s + (1-\beta_2) (\frac{\partial J}{\partial w_{jk}^{(i)}})^2 \\
w_{jk}^{(i)} \leftarrow w_{jk}^{(i)} - \frac{\alpha}{\sqrt{s+\epsilon}}\frac{\partial J}{\partial w_{jk}^{(i)}}
\end{displaymath}

\begin{figure}[!h]
\centering
\includegraphics[width=130pt]{"images/MLP/adam"}
\hspace*{10mm}
\includegraphics[width=130pt]{"images/MLP/adampropre"}
\caption{Exemple d'application de ADAM. La figure représente le cheminement de la valeur des deux poids. La hauteur représente l'erreur.L'algorithme est peu sensible aux minimum locaux et beaucoup plus stable que la méthode de descente de gradient newtonnienne.}
\label{ADAM}
\end{figure}

\subsection{Rétropropagation de l'erreur}

On a vu les principaux algorithmes de descente de gradient. Cependant cela suppose de connaître l'erreur commise par le vecteur à mettre à jour. Or, dans le cas d'un réseau de neurones, cette erreur est calculée à la sortie du réseau, c'est-à-dire après la dernière couche. Il faut donc trouver un moyen de calculer \textbf{l'influence} de chaque poids, de chaque couche dans l'erreur commise. En d'autre terme, il faut \textbf{rétro-propager l'erreur, de la sortie vers l'entrée}.







\subsubsection{Comparatif des algorithmes}

\begin{figure}[!h]
\centering
\includegraphics[width=130pt]{"images/MLP/comparatifoptimiseurs"}
\caption{Comparaison des 4 algorithmes présentés La figure représente le cheminement de la valeur des deux poids. La hauteur représente l'erreur. On observe que ADAM est clairement meilleur que les autres.}
\label{Comparatifoptimiseurs}
\end{figure}

\subsection{Les batchs}
Souvent la dérivée $\frac{\partial J}{\partial w_{jk}^{(i)}}$ n'est pas calculée sur une seule entrée. Afin d'obtenir une estimation plus précise du gradient de J, on le moyenne sur n entrées. On appelle cela la méthode par batchs. Il est bon de remarquer que le choix de n n'est pas anodin. Il s'agit ici encore d'un hyperparamètre à régler qui influe sur la convergence du réseau. L'influence de la taille des batchs sera étudiée plus loin.


