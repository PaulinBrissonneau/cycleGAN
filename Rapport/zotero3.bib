
@article{santurkar_how_nodate,
	title = {How Does Batch Normalization Help Optimization?},
	abstract = {Batch Normalization ({BatchNorm}) is a widely adopted technique that enables faster and more stable training of deep neural networks ({DNNs}). Despite its pervasiveness, the exact reasons for {BatchNorm}’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of {BatchNorm}. Instead, we uncover a more fundamental impact of {BatchNorm} on the training process: it makes the optimization landscape signiﬁcantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.},
	pages = {11},
	author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Ma, Aleksander},
	langid = {english},
	file = {Santurkar et al. - How Does Batch Normalization Help Optimization.pdf:/home/paulin/Zotero/storage/55YPJ4LV/Santurkar et al. - How Does Batch Normalization Help Optimization.pdf:application/pdf}
}

@article{arora_simple_nodate,
	title = {Simple, Eﬃcient, and Neural Algorithms for Sparse Coding},
	abstract = {Sparse coding is a basic task in many ﬁelds including signal processing, neuroscience and machine learning where the goal is to learn a basis that enables a sparse representation of a given set of data, if one exists. Its standard formulation is as a non-convex optimization problem which is solved in practice by heuristics based on alternating minimization. Recent work has resulted in several algorithms for sparse coding with provable guarantees, but somewhat surprisingly these are outperformed by the simple alternating minimization heuristics. Here we give a general framework for understanding alternating minimization which we leverage to analyze existing heuristics and to design new ones also with provable guarantees. Some of these algorithms seem implementable on simple neural architectures, which was the original motivation of Olshausen and Field (1997a) in introducing sparse coding. We also give the ﬁrst eﬃcient algorithm for sparse coding that works almost up to the information theoretic limit for sparse recovery on incoherent dictionaries. All previous algorithms that approached or surpassed this limit run in time exponential in some natural parameter. Finally, our algorithms improve upon the sample complexity of existing approaches. We believe that our analysis framework will have applications in other settings where simple iterative algorithms are used.},
	pages = {37},
	author = {Arora, Sanjeev and Ge, Rong and Ma, Tengyu and Moitra, Ankur},
	langid = {english},
	file = {Arora et al. - Simple, Eﬃcient, and Neural Algorithms for Sparse .pdf:/home/paulin/Zotero/storage/SLFT74SD/Arora et al. - Simple, Eﬃcient, and Neural Algorithms for Sparse .pdf:application/pdf}
}

@inproceedings{radford_unsupervised_2015,
	title = {Unsupervised representation learning with deep convolutional generative adversarial networks},
	url = {https://arxiv.org/abs/1511.06434},
	booktitle = {{arXiv} preprint {arXiv}:1511.06434},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	urldate = {2017-09-28},
	date = {2015},
	file = {1511.06434.pdf:/home/paulin/Zotero/storage/FDNUJIC9/1511.06434.pdf:application/pdf}
}

@article{arjovsky_wasserstein_2017,
	title = {Wasserstein gan},
	url = {https://arxiv.org/abs/1701.07875},
	journaltitle = {{arXiv} preprint {arXiv}:1701.07875},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
	urldate = {2017-09-28},
	date = {2017},
	file = {1701.07875.pdf:/home/paulin/Zotero/storage/8A5RZY2F/1701.07875.pdf:application/pdf}
}

@online{ruder_overview_2016,
	title = {An overview of gradient descent optimization algorithms},
	url = {http://ruder.io/optimizing-gradient-descent/index.html},
	author = {Ruder, Sebastian},
	date = {2016}
}

@inproceedings{salimans_improved_2016,
	title = {Improved techniques for training gans},
	url = {http://papers.nips.cc/paper/6124-improved-techniques-for-training-gans},
	pages = {2234--2242},
	booktitle = {Advances in Neural Information Processing Systems},
	author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
	urldate = {2017-09-28},
	date = {2016},
	file = {1606.03498.pdf:/home/paulin/Zotero/storage/X3VX38P6/1606.03498.pdf:application/pdf}
}

@online{olah_backpropagation_2015,
	title = {Backpropagation},
	url = {http://colah.github.io/posts/2015-08-Backprop/},
	author = {Olah, Christopher},
	date = {2015}
}

@inproceedings{lecun_efficient_1998,
	location = {London, {UK}, {UK}},
	title = {Efficient {BackProp}},
	isbn = {3-540-65311-2},
	url = {http://dl.acm.org/citation.cfm?id=645754.668382},
	pages = {9--50},
	booktitle = {Neural Networks: Tricks of the Trade, This Book is an Outgrowth of a 1996 {NIPS} Workshop},
	publisher = {Springer-Verlag},
	author = {{LeCun}, Yann and Bottou, Léon and Orr, Genevieve B. and Müller, Klaus-Robert},
	date = {1998},
	file = {lecun-98b[1].pdf:/home/paulin/Zotero/storage/HISWVIZD/lecun-98b[1].pdf:application/pdf}
}

@inproceedings{lecun_gradient-based_1998,
	title = {Gradient-Based Learning Applied to Document Recognition},
	volume = {86},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.7665},
	pages = {2278--2324},
	booktitle = {Proceedings of the {IEEE}},
	author = {{LeCun}, Yann and Bottou, Léon and Bengio, Yoshua and Haffner, Patrick},
	date = {1998},
	keywords = {{MSc} character\_recognition checked mnist network neural},
	file = {lecun-01a[1].pdf:/home/paulin/Zotero/storage/UASB9JWU/lecun-01a[1].pdf:application/pdf}
}

@inproceedings{goodfellow_nips_2016,
	title = {{NIPS} 2016 tutorial: Generative adversarial networks},
	url = {https://arxiv.org/abs/1701.00160},
	shorttitle = {{NIPS} 2016 tutorial},
	booktitle = {{arXiv} preprint {arXiv}:1701.00160},
	author = {Goodfellow, Ian},
	urldate = {2017-09-28},
	date = {2016},
	file = {1701.00160.pdf:/home/paulin/Zotero/storage/J7LV73KP/1701.00160.pdf:application/pdf}
}

@inproceedings{goodfellow_generative_2014,
	title = {Generative adversarial nets},
	url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets},
	shorttitle = {Goodfellow\_GAN\_2014},
	eventtitle = {{NIPS}},
	pages = {2672--2680},
	booktitle = {Advances in neural information processing systems},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	urldate = {2017-09-28},
	date = {2014},
	file = {5423-generative-adversarial-nets.pdf:/home/paulin/Zotero/storage/7STQ8VAK/5423-generative-adversarial-nets.pdf:application/pdf}
}

@article{park_adaptive_2019,
	title = {Adaptive Weighted Multi-Discriminator {CycleGAN} for Underwater Image Enhancement},
	volume = {7},
	issn = {2077-1312},
	url = {https://www.mdpi.com/2077-1312/7/7/200},
	doi = {10.3390/jmse7070200},
	abstract = {In this paper, we propose a novel underwater image enhancement method. Typical deep learning models for underwater image enhancement are trained by paired synthetic dataset. Therefore, these models are mostly effective for synthetic image enhancement but less so for real-world images. In contrast, cycle-consistent generative adversarial networks ({CycleGAN}) can be trained with unpaired dataset. However, performance of the {CycleGAN} is highly dependent upon the dataset, thus it may generate unrealistic images with less content information than original images. A novel solution we propose here is by starting with a {CycleGAN}, we add a pair of discriminators to preserve contents of input image while enhancing the image. As a part of the solution, we introduce an adaptive weighting method for limiting losses of the two types of discriminators to balance their inﬂuence and stabilize the training procedure. Extensive experiments demonstrate that the proposed method signiﬁcantly outperforms the state-of-the-art methods on real-world underwater images.},
	pages = {200},
	number = {7},
	journaltitle = {Journal of Marine Science and Engineering},
	shortjournal = {{JMSE}},
	author = {Park, Jaihyun and Han, David K. and Ko, Hanseok},
	urldate = {2019-11-13},
	date = {2019-06-28},
	langid = {english},
	file = {Park et al. - 2019 - Adaptive Weighted Multi-Discriminator CycleGAN for.pdf:/home/paulin/Zotero/storage/845GEJ8T/Park et al. - 2019 - Adaptive Weighted Multi-Discriminator CycleGAN for.pdf:application/pdf}
}

@article{santurkar_how_nodate-1,
	title = {How Does Batch Normalization Help Optimization?},
	abstract = {Batch Normalization ({BatchNorm}) is a widely adopted technique that enables faster and more stable training of deep neural networks ({DNNs}). Despite its pervasiveness, the exact reasons for {BatchNorm}’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of {BatchNorm}. Instead, we uncover a more fundamental impact of {BatchNorm} on the training process: it makes the optimization landscape signiﬁcantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.},
	pages = {11},
	author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Ma, Aleksander},
	langid = {english},
	file = {Santurkar et al. - How Does Batch Normalization Help Optimization.pdf:/home/paulin/Zotero/storage/LSBXE5ZW/Santurkar et al. - How Does Batch Normalization Help Optimization.pdf:application/pdf}
}

@article{zhu_unpaired_2018,
	title = {Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},
	url = {http://arxiv.org/abs/1703.10593},
	abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y → X and introduce a cycle consistency loss to enforce F (G(X)) ≈ X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transﬁguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
	journaltitle = {{arXiv}:1703.10593 [cs]},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	urldate = {2019-11-13},
	date = {2018-11-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1703.10593},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Zhu et al. - 2018 - Unpaired Image-to-Image Translation using Cycle-Co.pdf:/home/paulin/Zotero/storage/HXXVQI8H/Zhu et al. - 2018 - Unpaired Image-to-Image Translation using Cycle-Co.pdf:application/pdf}
}

@article{rolnick_tackling_2019,
	title = {Tackling Climate Change with Machine Learning},
	url = {http://arxiv.org/abs/1906.05433},
	abstract = {Climate change is one of the greatest challenges facing humanity, and we, as machine learning experts, may wonder how we can help. Here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by machine learning, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the machine learning community to join the global effort against climate change.},
	journaltitle = {{arXiv}:1906.05433 [cs, stat]},
	author = {Rolnick, David and Donti, Priya L. and Kaack, Lynn H. and Kochanski, Kelly and Lacoste, Alexandre and Sankaran, Kris and Ross, Andrew Slavin and Milojevic-Dupont, Nikola and Jaques, Natasha and Waldman-Brown, Anna and Luccioni, Alexandra and Maharaj, Tegan and Sherwin, Evan D. and Mukkavilli, S. Karthik and Kording, Konrad P. and Gomes, Carla and Ng, Andrew Y. and Hassabis, Demis and Platt, John C. and Creutzig, Felix and Chayes, Jennifer and Bengio, Yoshua},
	urldate = {2020-04-08},
	date = {2019-11-05},
	eprinttype = {arxiv},
	eprint = {1906.05433},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	file = {arXiv.org Snapshot:/home/paulin/Zotero/storage/4D9FK2RE/1906.html:text/html;arXiv Fulltext PDF:/home/paulin/Zotero/storage/S3NS7UVF/Rolnick et al. - 2019 - Tackling Climate Change with Machine Learning.pdf:application/pdf}
}

@article{gulrajani_improved_2017,
	title = {Improved Training of Wasserstein {GANs}},
	url = {http://arxiv.org/abs/1704.00028},
	abstract = {Generative Adversarial Networks ({GANs}) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein {GAN} ({WGAN}) makes progress toward stable training of {GANs}, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in {WGAN} to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard {WGAN} and enables stable training of a wide variety of {GAN} architectures with almost no hyperparameter tuning, including 101-layer {ResNets} and language models over discrete data. We also achieve high quality generations on {CIFAR}-10 and {LSUN} bedrooms.},
	journaltitle = {{arXiv}:1704.00028 [cs, stat]},
	author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
	urldate = {2020-03-01},
	date = {2017-12-25},
	eprinttype = {arxiv},
	eprint = {1704.00028},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/paulin/Zotero/storage/JF7FKZW5/1704.html:text/html;arXiv Fulltext PDF:/home/paulin/Zotero/storage/WER3ZD7F/Gulrajani et al. - 2017 - Improved Training of Wasserstein GANs.pdf:application/pdf}
}

@article{borji_pros_2018,
	title = {Pros and Cons of {GAN} Evaluation Measures},
	url = {http://arxiv.org/abs/1802.03446},
	abstract = {Generative models, in particular generative adversarial networks ({GANs}), have received significant attention recently. A number of {GAN} variants have been proposed and have been utilized in many applications. Despite large strides in terms of theoretical progress, evaluating and comparing {GANs} remains a daunting task. While several measures have been introduced, as of yet, there is no consensus as to which measure best captures strengths and limitations of models and should be used for fair model comparison. As in other areas of computer vision and machine learning, it is critical to settle on one or few good measures to steer the progress in this field. In this paper, I review and critically discuss more than 24 quantitative and 5 qualitative measures for evaluating generative models with a particular emphasis on {GAN}-derived models. I also provide a set of 7 desiderata followed by an evaluation of whether a given measure or a family of measures is compatible with them.},
	journaltitle = {{arXiv}:1802.03446 [cs]},
	author = {Borji, Ali},
	urldate = {2020-02-28},
	date = {2018-10-23},
	eprinttype = {arxiv},
	eprint = {1802.03446},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/paulin/Zotero/storage/EJUXHVUA/1802.html:text/html;arXiv Fulltext PDF:/home/paulin/Zotero/storage/E5KC6Y8B/Borji - 2018 - Pros and Cons of GAN Evaluation Measures.pdf:application/pdf}
}

@article{szegedy_rethinking_2015,
	title = {Rethinking the Inception Architecture for Computer Vision},
	url = {http://arxiv.org/abs/1512.00567},
	abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the {ILSVRC} 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5 error on the validation set (3.6\% error on the test set) and 17.3\% top-1 error on the validation set.},
	journaltitle = {{arXiv}:1512.00567 [cs]},
	author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
	urldate = {2020-02-27},
	date = {2015-12-11},
	eprinttype = {arxiv},
	eprint = {1512.00567},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/paulin/Zotero/storage/L9IPLPCF/1512.html:text/html;arXiv Fulltext PDF:/home/paulin/Zotero/storage/JZ8I58K2/Szegedy et al. - 2015 - Rethinking the Inception Architecture for Computer.pdf:application/pdf}
}

@article{barratt_note_2018,
	title = {A Note on the Inception Score},
	url = {http://arxiv.org/abs/1801.01973},
	abstract = {Deep generative models are powerful tools that have produced impressive results in recent years. These advances have been for the most part empirically driven, making it essential that we use high quality evaluation metrics. In this paper, we provide new insights into the Inception Score, a recently proposed and widely used evaluation metric for generative models, and demonstrate that it fails to provide useful guidance when comparing models. We discuss both suboptimalities of the metric itself and issues with its application. Finally, we call for researchers to be more systematic and careful when evaluating and comparing generative models, as the advancement of the field depends upon it.},
	journaltitle = {{arXiv}:1801.01973 [cs, stat]},
	author = {Barratt, Shane and Sharma, Rishi},
	urldate = {2020-02-27},
	date = {2018-06-21},
	eprinttype = {arxiv},
	eprint = {1801.01973},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/paulin/Zotero/storage/7Z43M2LJ/1801.html:text/html;arXiv Fulltext PDF:/home/paulin/Zotero/storage/NUZJV3ZW/Barratt et Sharma - 2018 - A Note on the Inception Score.pdf:application/pdf}
}

@article{salimans_improved_2016-1,
	title = {Improved Techniques for Training {GANs}},
	url = {http://arxiv.org/abs/1606.03498},
	abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks ({GANs}) framework. We focus on two applications of {GANs}: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on {MNIST}, {CIFAR}-10 and {SVHN}. The generated images are of high quality as confirmed by a visual Turing test: our model generates {MNIST} samples that humans cannot distinguish from real data, and {CIFAR}-10 samples that yield a human error rate of 21.3\%. We also present {ImageNet} samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of {ImageNet} classes.},
	journaltitle = {{arXiv}:1606.03498 [cs]},
	author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
	urldate = {2020-02-27},
	date = {2016-06-10},
	eprinttype = {arxiv},
	eprint = {1606.03498},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:/home/paulin/Zotero/storage/U8IRMPAN/1606.html:text/html;arXiv Fulltext PDF:/home/paulin/Zotero/storage/AHKWUPB7/Salimans et al. - 2016 - Improved Techniques for Training GANs.pdf:application/pdf}
}

@article{heusel_gans_2018,
	title = {{GANs} Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
	url = {http://arxiv.org/abs/1706.08500},
	abstract = {Generative Adversarial Networks ({GANs}) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of {GAN} training has still not been proved. We propose a two time-scale update rule ({TTUR}) for training {GANs} with stochastic gradient descent on arbitrary {GAN} loss functions. {TTUR} has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the {TTUR} converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of {GANs} at image generation, we introduce the "Fr{\textbackslash}'echet Inception Distance" ({FID}) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, {TTUR} improves learning for {DCGANs} and Improved Wasserstein {GANs} ({WGAN}-{GP}) outperforming conventional {GAN} training on {CelebA}, {CIFAR}-10, {SVHN}, {LSUN} Bedrooms, and the One Billion Word Benchmark.},
	journaltitle = {{arXiv}:1706.08500 [cs, stat]},
	author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
	urldate = {2020-02-27},
	date = {2018-01-12},
	eprinttype = {arxiv},
	eprint = {1706.08500},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/paulin/Zotero/storage/4PDTSG8I/1706.html:text/html;arXiv Fulltext PDF:/home/paulin/Zotero/storage/95GJQ44J/Heusel et al. - 2018 - GANs Trained by a Two Time-Scale Update Rule Conve.pdf:application/pdf}
}

@inproceedings{gatys_image_2016,
	location = {Las Vegas, {NV}, {USA}},
	title = {Image Style Transfer Using Convolutional Neural Networks},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780634/},
	doi = {10.1109/CVPR.2016.265},
	abstract = {Rendering the semantic content of an image in different styles is a difﬁcult image processing task. Arguably, a major limiting factor for previous approaches has been the lack of image representations that explicitly represent semantic information and, thus, allow to separate image content from style. Here we use image representations derived from Convolutional Neural Networks optimised for object recognition, which make high level image information explicit. We introduce A Neural Algorithm of Artistic Style that can separate and recombine the image content and style of natural images. The algorithm allows us to produce new images of high perceptual quality that combine the content of an arbitrary photograph with the appearance of numerous wellknown artworks. Our results provide new insights into the deep image representations learned by Convolutional Neural Networks and demonstrate their potential for high level image synthesis and manipulation.},
	eventtitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {2414--2423},
	booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
	urldate = {2020-06-05},
	date = {2016-06},
	langid = {english},
	file = {Gatys et al. - 2016 - Image Style Transfer Using Convolutional Neural Ne.pdf:/home/paulin/Zotero/storage/MMR8BKDI/Gatys et al. - 2016 - Image Style Transfer Using Convolutional Neural Ne.pdf:application/pdf}
}

@article{isola_image--image_2018,
	title = {Image-to-Image Translation with Conditional Adversarial Networks},
	url = {http://arxiv.org/abs/1611.07004},
	abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
	journaltitle = {{arXiv}:1611.07004 [cs]},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	urldate = {2020-06-05},
	date = {2018-11-26},
	eprinttype = {arxiv},
	eprint = {1611.07004},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/paulin/Zotero/storage/JG9CGDCB/Isola et al. - 2018 - Image-to-Image Translation with Conditional Advers.pdf:application/pdf;arXiv.org Snapshot:/home/paulin/Zotero/storage/S4TMYVZ4/1611.html:text/html}
}

@article{kingma_adam_2017,
	title = {Adam: A Method for Stochastic Optimization},
	url = {http://arxiv.org/abs/1412.6980},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for ﬁrst-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efﬁcient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the inﬁnity norm.},
	journaltitle = {{arXiv}:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	urldate = {2020-06-05},
	date = {2017-01-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1412.6980},
	keywords = {Computer Science - Machine Learning},
	file = {Kingma et Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:/home/paulin/Zotero/storage/J4RALTXZ/Kingma et Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf}
}

@article{shrivastava_learning_2017,
	title = {Learning from Simulated and Unsupervised Images through Adversarial Training},
	url = {http://arxiv.org/abs/1612.07828},
	abstract = {With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulator’s output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks ({GANs}), but with synthetic images as inputs instead of random vectors. We make several key modiﬁcations to the standard {GAN} algorithm to preserve annotations, avoid artifacts, and stabilize training: (i) a ‘self-regularization’ term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of reﬁned images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a signiﬁcant improvement over using synthetic images, and achieve state-of-the-art results on the {MPIIGaze} dataset without any labeled real data.},
	journaltitle = {{arXiv}:1612.07828 [cs]},
	author = {Shrivastava, Ashish and Pfister, Tomas and Tuzel, Oncel and Susskind, Josh and Wang, Wenda and Webb, Russ},
	urldate = {2020-06-05},
	date = {2017-07-19},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1612.07828},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Shrivastava et al. - 2017 - Learning from Simulated and Unsupervised Images th.pdf:/home/paulin/Zotero/storage/QC87RDM2/Shrivastava et al. - 2017 - Learning from Simulated and Unsupervised Images th.pdf:application/pdf}
}

@article{isola_image--image_2018-1,
	title = {Image-to-Image Translation with Conditional Adversarial Networks},
	url = {http://arxiv.org/abs/1611.07004},
	abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
	journaltitle = {{arXiv}:1611.07004 [cs]},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	urldate = {2020-06-05},
	date = {2018-11-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1611.07004},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Isola et al. - 2018 - Image-to-Image Translation with Conditional Advers.pdf:/home/paulin/Zotero/storage/UNDTRLWN/Isola et al. - 2018 - Image-to-Image Translation with Conditional Advers.pdf:application/pdf}
}

@article{isola_image--image_2018-2,
	title = {Image-to-Image Translation with Conditional Adversarial Networks},
	url = {http://arxiv.org/abs/1611.07004},
	abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
	journaltitle = {{arXiv}:1611.07004 [cs]},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	urldate = {2020-06-05},
	date = {2018-11-26},
	eprinttype = {arxiv},
	eprint = {1611.07004},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/paulin/Zotero/storage/6TSN5MG5/Isola et al. - 2018 - Image-to-Image Translation with Conditional Advers.pdf:application/pdf;arXiv.org Snapshot:/home/paulin/Zotero/storage/99FJAPNV/1611.html:text/html}
}

@article{isola_image--image_2018-3,
	title = {Image-to-Image Translation with Conditional Adversarial Networks},
	url = {http://arxiv.org/abs/1611.07004},
	abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
	journaltitle = {{arXiv}:1611.07004 [cs]},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	urldate = {2020-06-05},
	date = {2018-11-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1611.07004},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Isola et al. - 2018 - Image-to-Image Translation with Conditional Advers.pdf:/home/paulin/Zotero/storage/NR2LSASR/Isola et al. - 2018 - Image-to-Image Translation with Conditional Advers.pdf:application/pdf}
}

@article{ulyanov_instance_2017,
	title = {Instance Normalization: The Missing Ingredient for Fast Stylization},
	url = {http://arxiv.org/abs/1607.08022},
	shorttitle = {Instance Normalization},
	abstract = {It this paper we revisit the fast stylization method introduced in Ulyanov et al. (2016). We show how a small change in the stylization architecture results in a signiﬁcant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code is available at https://github.com/{DmitryUlyanov}/texture\_nets. Full paper can be found at https://arxiv.org/abs/1701.02096.},
	journaltitle = {{arXiv}:1607.08022 [cs]},
	author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
	urldate = {2020-06-05},
	date = {2017-11-06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1607.08022},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Ulyanov et al. - 2017 - Instance Normalization The Missing Ingredient for.pdf:/home/paulin/Zotero/storage/IJNB3DYE/Ulyanov et al. - 2017 - Instance Normalization The Missing Ingredient for.pdf:application/pdf}
}

@article{ulyanov_instance_2017-1,
	title = {Instance Normalization: The Missing Ingredient for Fast Stylization},
	url = {http://arxiv.org/abs/1607.08022},
	shorttitle = {Instance Normalization},
	abstract = {It this paper we revisit the fast stylization method introduced in Ulyanov et al. (2016). We show how a small change in the stylization architecture results in a signiﬁcant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code is available at https://github.com/{DmitryUlyanov}/texture\_nets. Full paper can be found at https://arxiv.org/abs/1701.02096.},
	journaltitle = {{arXiv}:1607.08022 [cs]},
	author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
	urldate = {2020-06-05},
	date = {2017-11-06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1607.08022},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Ulyanov et al. - 2017 - Instance Normalization The Missing Ingredient for.pdf:/home/paulin/Zotero/storage/GFDAY47H/Ulyanov et al. - 2017 - Instance Normalization The Missing Ingredient for.pdf:application/pdf}
}

@article{he_deep_2015,
	title = {Deep Residual Learning for Image Recognition},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the {ImageNet} dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than {VGG} nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the {ImageNet} test set. This result won the 1st place on the {ILSVRC} 2015 classification task. We also present analysis on {CIFAR}-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the {COCO} object detection dataset. Deep residual nets are foundations of our submissions to {ILSVRC} \& {COCO} 2015 competitions, where we also won the 1st places on the tasks of {ImageNet} detection, {ImageNet} localization, {COCO} detection, and {COCO} segmentation.},
	journaltitle = {{arXiv}:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	urldate = {2020-06-05},
	date = {2015-12-10},
	eprinttype = {arxiv},
	eprint = {1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/paulin/Zotero/storage/IHQ59HSK/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/home/paulin/Zotero/storage/3XPKFQTT/1512.html:text/html}
}

@article{kingma_adam_2017-1,
	title = {Adam: A Method for Stochastic Optimization},
	url = {http://arxiv.org/abs/1412.6980},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the infinity norm.},
	journaltitle = {{arXiv}:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	urldate = {2020-06-05},
	date = {2017-01-29},
	eprinttype = {arxiv},
	eprint = {1412.6980},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/paulin/Zotero/storage/X8YFENEK/Kingma et Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/home/paulin/Zotero/storage/B4GBT774/1412.html:text/html}
}

@article{goodfellow_generative_2014-1,
	title = {Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	journaltitle = {{arXiv}:1406.2661 [cs, stat]},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	urldate = {2020-06-05},
	date = {2014-06-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1406.2661},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:/home/paulin/Zotero/storage/BK5W483Q/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf}
}

@inproceedings{kalal_forward-backward_2010,
	location = {{USA}},
	title = {Forward-Backward Error: Automatic Detection of Tracking Failures},
	isbn = {978-0-7695-4109-9},
	url = {https://doi.org/10.1109/ICPR.2010.675},
	doi = {10.1109/ICPR.2010.675},
	series = {{ICPR} '10},
	shorttitle = {Forward-Backward Error},
	abstract = {This paper proposes a novel method for tracking failure detection. The detection is based on the Forward-Backward error, i.e. the tracking is performed forward and backward in time and the discrepancies between these two trajectories are measured. We demonstrate that the proposed error enables reliable detection of tracking failures and selection of reliable trajectories in video sequences. We demonstrate that the approach is complementary to commonly used normalized cross-correlation ({NCC}). Based on the error, we propose a novel object tracker called Median Flow. State-of-the-art performance is achieved on challenging benchmark video sequences which include non-rigid objects.},
	pages = {2756--2759},
	booktitle = {Proceedings of the 2010 20th International Conference on Pattern Recognition},
	publisher = {{IEEE} Computer Society},
	author = {Kalal, Zdenek and Mikolajczyk, Krystian and Matas, Jiri},
	urldate = {2020-06-05},
	date = {2010-08-23},
	keywords = {forward backward error, tracking failure detection}
}

@article{taigman_unsupervised_2016,
	title = {Unsupervised Cross-Domain Image Generation},
	url = {http://arxiv.org/abs/1611.02200},
	abstract = {We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given function f, which accepts inputs in either domains, would remain unchanged. Other than the function f, the training data is unsupervised and consist of a set of samples from each domain. The Domain Transfer Network ({DTN}) we present employs a compound loss function that includes a multiclass {GAN} loss, an f-constancy component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.},
	journaltitle = {{arXiv}:1611.02200 [cs]},
	author = {Taigman, Yaniv and Polyak, Adam and Wolf, Lior},
	urldate = {2020-06-05},
	date = {2016-11-07},
	eprinttype = {arxiv},
	eprint = {1611.02200},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/paulin/Zotero/storage/FZYIFHS2/Taigman et al. - 2016 - Unsupervised Cross-Domain Image Generation.pdf:application/pdf;arXiv.org Snapshot:/home/paulin/Zotero/storage/95CGGSFA/1611.html:text/html}
}

@article{johnson_perceptual_2016,
	title = {Perceptual Losses for Real-Time Style Transfer and Super-Resolution},
	url = {http://arxiv.org/abs/1603.08155},
	abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a {\textbackslash}emph\{per-pixel\} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing {\textbackslash}emph\{perceptual\} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
	journaltitle = {{arXiv}:1603.08155 [cs]},
	author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
	urldate = {2020-06-05},
	date = {2016-03-26},
	eprinttype = {arxiv},
	eprint = {1603.08155},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/paulin/Zotero/storage/3997YU98/Johnson et al. - 2016 - Perceptual Losses for Real-Time Style Transfer and.pdf:application/pdf;arXiv.org Snapshot:/home/paulin/Zotero/storage/F34V9GMN/1603.html:text/html}
}

@article{shrivastava_learning_2016,
	title = {Learning from Simulated and Unsupervised Images through Adversarial Training},
	url = {https://arxiv.org/abs/1612.07828v2},
	abstract = {With recent progress in graphics, it has become more tractable to train
models on synthetic images, potentially avoiding the need for expensive
annotations. However, learning from synthetic images may not achieve the
desired performance due to a gap between synthetic and real image
distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U)
learning, where the task is to learn a model to improve the realism of a
simulator's output using unlabeled real data, while preserving the annotation
information from the simulator. We develop a method for S+U learning that uses
an adversarial network similar to Generative Adversarial Networks ({GANs}), but
with synthetic images as inputs instead of random vectors. We make several key
modifications to the standard {GAN} algorithm to preserve annotations, avoid
artifacts, and stabilize training: (i) a 'self-regularization' term, (ii) a
local adversarial loss, and (iii) updating the discriminator using a history of
refined images. We show that this enables generation of highly realistic
images, which we demonstrate both qualitatively and with a user study. We
quantitatively evaluate the generated images by training models for gaze
estimation and hand pose estimation. We show a significant improvement over
using synthetic images, and achieve state-of-the-art results on the {MPIIGaze}
dataset without any labeled real data.},
	author = {Shrivastava, Ashish and Pfister, Tomas and Tuzel, Oncel and Susskind, Josh and Wang, Wenda and Webb, Russ},
	urldate = {2020-06-05},
	date = {2016-12-22},
	langid = {english},
	file = {Full Text PDF:/home/paulin/Zotero/storage/YXTLMM9G/Shrivastava et al. - 2016 - Learning from Simulated and Unsupervised Images th.pdf:application/pdf;Snapshot:/home/paulin/Zotero/storage/458NDSBS/1612.html:text/html}
}

@article{goldsborough_tour_2016,
	title = {A Tour of {TensorFlow}},
	url = {http://arxiv.org/abs/1610.01178},
	abstract = {Deep learning is a branch of artiﬁcial intelligence employing deep neural network architectures that has signiﬁcantly advanced the state-of-the-art in computer vision, speech recognition, natural language processing and other domains. In November 2015, Google released {TensorFlow}, an open source deep learning software library for deﬁning, training and deploying machine learning models. In this paper, we review {TensorFlow} and put it in context of modern deep learning concepts and software. We discuss its basic computational paradigms and distributed execution model, its programming interface as well as accompanying visualization toolkits. We then compare {TensorFlow} to alternative libraries such as Theano, Torch or Caffe on a qualitative as well as quantitative basis and ﬁnally comment on observed use-cases of {TensorFlow} in academia and industry.},
	journaltitle = {{arXiv}:1610.01178 [cs]},
	author = {Goldsborough, Peter},
	urldate = {2020-06-06},
	date = {2016-10-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1610.01178},
	keywords = {Computer Science - Machine Learning},
	file = {Goldsborough - 2016 - A Tour of TensorFlow.pdf:/home/paulin/Zotero/storage/H22WP67C/Goldsborough - 2016 - A Tour of TensorFlow.pdf:application/pdf}
}

@article{mcculloch_logical_1943,
	title = {A {LOGICAL} {CALCULUS} {OF} {THE} {IDEAS} {IMMANENT} {IN} {NERVOUS} {ACTIVITY}},
	volume = {5},
	pages = {115--133},
	journaltitle = {Bulletin of Mathematical Biophysics},
	author = {Mcculloch, Warren S and Pitts, Walter},
	date = {1943},
	langid = {english},
	file = {Mcculloch et Pitts - A LOGICAL CALCULUS OF THE IDEAS IMMANENT IN NERVOU.pdf:/home/paulin/Zotero/storage/Y9EX95LV/Mcculloch et Pitts - A LOGICAL CALCULUS OF THE IDEAS IMMANENT IN NERVOU.pdf:application/pdf}
}

@article{mcculloch_logical_1943-1,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	issn = {1522-9602},
	url = {https://doi.org/10.1007/BF02478259},
	doi = {10.1007/BF02478259},
	abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	pages = {115--133},
	number = {4},
	journaltitle = {The bulletin of mathematical biophysics},
	shortjournal = {The bulletin of mathematical biophysics},
	author = {{McCulloch}, Warren S. and Pitts, Walter},
	date = {1943-12-01}
}

@article{noauthor_notitle_nodate
}

@article{van_der_maaten_visualizing_2008,
	title = {Visualizing data using t-{SNE}},
	url = {http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf},
	series = {{JMLR}},
	journaltitle = {{JMLR} 9},
	author = {van der Maaten, Laurens and Hinton, Geoffrey},
	date = {2008-08-11}
}

@book{villani_optimal_2006,
	title = {Optimal Transport: Old and New},
	isbn = {978-3-540-71050-9},
	url = {https://ljk.imag.fr/membres/Emmanuel.Maitre/lib/exe/fetch.php?media=b07.stflour.pdf},
	pagetotal = {635},
	author = {Villani, Cédric},
	date = {2006-12-22},
	langid = {english}
}

@article{radford_unsupervised_2015-1,
	title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
	url = {https://arxiv.org/abs/1511.06434v2},
	abstract = {In recent years, supervised learning with convolutional networks ({CNNs}) has
seen huge adoption in computer vision applications. Comparatively, unsupervised
learning with {CNNs} has received less attention. In this work we hope to help
bridge the gap between the success of {CNNs} for supervised learning and
unsupervised learning. We introduce a class of {CNNs} called deep convolutional
generative adversarial networks ({DCGANs}), that have certain architectural
constraints, and demonstrate that they are a strong candidate for unsupervised
learning. Training on various image datasets, we show convincing evidence that
our deep convolutional adversarial pair learns a hierarchy of representations
from object parts to scenes in both the generator and discriminator.
Additionally, we use the learned features for novel tasks - demonstrating their
applicability as general image representations.},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	urldate = {2020-06-06},
	date = {2015-11-19},
	langid = {english},
	file = {Full Text PDF:/home/paulin/Zotero/storage/3T7YSNCR/Radford et al. - 2015 - Unsupervised Representation Learning with Deep Con.pdf:application/pdf;Snapshot:/home/paulin/Zotero/storage/F7J7Z6JA/1511.html:text/html}
}

@article{radford_unsupervised_2016,
	title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks ({CNNs}) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with {CNNs} has received less attention. In this work we hope to help bridge the gap between the success of {CNNs} for supervised learning and unsupervised learning. We introduce a class of {CNNs} called deep convolutional generative adversarial networks ({DCGANs}), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	journaltitle = {{arXiv}:1511.06434 [cs]},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	urldate = {2020-06-06},
	date = {2016-01-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1511.06434},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf:/home/paulin/Zotero/storage/MFVTM986/Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf:application/pdf}
}

@article{long_fully_2014,
	title = {Fully Convolutional Networks for Semantic Segmentation},
	url = {https://arxiv.org/abs/1411.4038v2},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of
features. We show that convolutional networks by themselves, trained
end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic
segmentation. Our key insight is to build "fully convolutional" networks that
take input of arbitrary size and produce correspondingly-sized output with
efficient inference and learning. We define and detail the space of fully
convolutional networks, explain their application to spatially dense prediction
tasks, and draw connections to prior models. We adapt contemporary
classification networks ({AlexNet}, the {VGG} net, and {GoogLeNet}) into fully
convolutional networks and transfer their learned representations by
fine-tuning to the segmentation task. We then define a novel architecture that
combines semantic information from a deep, coarse layer with appearance
information from a shallow, fine layer to produce accurate and detailed
segmentations. Our fully convolutional network achieves state-of-the-art
segmentation of {PASCAL} {VOC} (20\% relative improvement to 62.2\% mean {IU} on 2012),
{NYUDv}2, and {SIFT} Flow, while inference takes one third of a second for a
typical image.},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	urldate = {2020-06-06},
	date = {2014-11-14},
	langid = {english},
	file = {Full Text PDF:/home/paulin/Zotero/storage/SVVKCEPG/Long et al. - 2014 - Fully Convolutional Networks for Semantic Segmenta.pdf:application/pdf;Snapshot:/home/paulin/Zotero/storage/3X97ZBNB/1411.html:text/html}
}

@article{ruder_overview_2017,
	title = {An overview of gradient descent optimization algorithms},
	url = {http://arxiv.org/abs/1609.04747},
	abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
	journaltitle = {{arXiv}:1609.04747 [cs]},
	author = {Ruder, Sebastian},
	urldate = {2020-06-06},
	date = {2017-06-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1609.04747},
	keywords = {Computer Science - Machine Learning},
	file = {Ruder - 2017 - An overview of gradient descent optimization algor.pdf:/home/paulin/Zotero/storage/SUVS5V2B/Ruder - 2017 - An overview of gradient descent optimization algor.pdf:application/pdf}
}

@article{rosenblatt_perceptron_nodate,
	title = {The perceptron: a probabilistic model for information storage and organization in the brain.},
	author = {Rosenblatt, Frank}
}

@article{rosenblatt_perceptron_1958,
	title = {The perceptron: A probabilistic model for information storage and organization in the brain.},
	volume = {65},
	issn = {1939-1471(Electronic),0033-295X(Print)},
	doi = {10.1037/h0042519},
	abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {386--408},
	number = {6},
	journaltitle = {Psychological Review},
	author = {Rosenblatt, F.},
	date = {1958},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {*Brain, *Cognition, *Memory, Nervous System}
}

@article{srivastava_dropout_nodate,
	title = {Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overﬁtting is a serious problem in such networks. Large networks are also slow to use, making it diﬃcult to deal with overﬁtting by combining the predictions of many diﬀerent large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of diﬀerent “thinned” networks. At test time, it is easy to approximate the eﬀect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This signiﬁcantly reduces overﬁtting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classiﬁcation and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	pages = {30},
	author = {Srivastava, Nitish and Hinton, Geoﬀrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	langid = {english},
	file = {Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf:/home/paulin/Zotero/storage/UXXLPY5C/Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf:application/pdf}
}

@online{verma_understanding_2020,
	title = {Understanding 1D and 3D Convolution Neural Network {\textbar} Keras},
	url = {https://towardsdatascience.com/understanding-1d-and-3d-convolution-neural-network-keras-9d8f76e29610},
	abstract = {When we say Convolution Neural Network ({CNN}), generally we refer to a 2 dimensional {CNN} which is used for image classification. But there…},
	titleaddon = {Medium},
	author = {Verma, Shiva},
	urldate = {2020-06-06},
	date = {2020-05-18},
	langid = {english},
	note = {Library Catalog: towardsdatascience.com}
}

@article{dumoulin_guide_2018,
	title = {A guide to convolution arithmetic for deep learning},
	url = {http://arxiv.org/abs/1603.07285},
	abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
	journaltitle = {{arXiv}:1603.07285 [cs, stat]},
	author = {Dumoulin, Vincent and Visin, Francesco},
	urldate = {2020-06-06},
	date = {2018-01-11},
	eprinttype = {arxiv},
	eprint = {1603.07285},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/paulin/Zotero/storage/JMZNAD7X/1603.html:text/html;arXiv Fulltext PDF:/home/paulin/Zotero/storage/TGUVSRUR/Dumoulin et Visin - 2018 - A guide to convolution arithmetic for deep learnin.pdf:application/pdf}
}

@article{szegedy_going_2014,
	title = {Going Deeper with Convolutions},
	url = {http://arxiv.org/abs/1409.4842},
	abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the {ImageNet} Large-Scale Visual Recognition Challenge 2014 ({ILSVRC} 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for {ILSVRC} 2014 is called {GoogLeNet}, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
	journaltitle = {{arXiv}:1409.4842 [cs]},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	urldate = {2020-06-06},
	date = {2014-09-16},
	eprinttype = {arxiv},
	eprint = {1409.4842},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/paulin/Zotero/storage/9RH4ESDW/1409.html:text/html;arXiv Fulltext PDF:/home/paulin/Zotero/storage/GATP7WUS/Szegedy et al. - 2014 - Going Deeper with Convolutions.pdf:application/pdf}
}

@incollection{krizhevsky_imagenet_2012,
	title = {{ImageNet} Classification with Deep Convolutional Neural Networks},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
	pages = {1097--1105},
	booktitle = {Advances in Neural Information Processing Systems 25},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	urldate = {2020-06-06},
	date = {2012},
	file = {NIPS Snapshot:/home/paulin/Zotero/storage/N325LURC/4824-imagenet-classification-with-deep-convolutional-neural-networks.html:text/html;NIPS Full Text PDF:/home/paulin/Zotero/storage/39I4BH4B/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf}
}

@article{fukushima_neocognitron_1980,
	title = {Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position},
	volume = {36},
	issn = {0340-1200, 1432-0770},
	url = {http://link.springer.com/10.1007/BF00344251},
	doi = {10.1007/BF00344251},
	shorttitle = {Neocognitron},
	abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by "learning without a teacher", and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname "neocognitron". After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consists of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of "S-cells', which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of "C-cells" similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any "teacher" during the process of selforganization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cells of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.},
	pages = {193--202},
	number = {4},
	journaltitle = {Biological Cybernetics},
	shortjournal = {Biol. Cybernetics},
	author = {Fukushima, Kunihiko},
	urldate = {2020-06-05},
	date = {1980-04},
	langid = {english},
	file = {Fukushima - 1980 - Neocognitron A self-organizing neural network mod.pdf:/home/paulin/Zotero/storage/PYKVBNEW/Fukushima - 1980 - Neocognitron A self-organizing neural network mod.pdf:application/pdf}
}

@inproceedings{godfrey_continuum_2015,
	title = {A continuum among logarithmic, linear, and exponential functions, and its potential to improve generalization in neural networks},
	volume = {1},
	isbn = {989-758-164-2},
	abstract = {{SoftExponential}},
	eventtitle = {2015 7th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management ({IC}3K)},
	pages = {481--486},
	publisher = {{IEEE}},
	author = {Godfrey, Luke B and Gashler, Michael S},
	date = {2015}
}

@article{misra_mish_2019,
	title = {Mish: A Self Regularized Non-Monotonic Neural Activation Function},
	journaltitle = {{arXiv} preprint {arXiv}:1908.08681},
	shortjournal = {{arXiv} preprint {arXiv}:1908.08681},
	author = {Misra, Diganta},
	date = {2019}
}

@article{lillicrap_backpropagation_2020,
	title = {Backpropagation and the brain},
	volume = {21},
	issn = {1471-003X, 1471-0048},
	url = {http://www.nature.com/articles/s41583-020-0277-3},
	doi = {10.1038/s41583-020-0277-3},
	pages = {335--346},
	number = {6},
	journaltitle = {Nature Reviews Neuroscience},
	shortjournal = {Nat Rev Neurosci},
	author = {Lillicrap, Timothy P. and Santoro, Adam and Marris, Luke and Akerman, Colin J. and Hinton, Geoffrey},
	urldate = {2020-06-07},
	date = {2020-06},
	langid = {english}
}

@article{hadjar_etude_2020,
	title = {Une étude de l'évolutivité des modèles pour la reconnaissance de documents arabes dans un contexte interactif},
	abstract = {Cette thèse aborde la reconnaissance de structures physiques et logiques de documents complexes, riches en variabilité. Plus particulièrement, nous avons étudié l’évolutivité des modèles dans un contexte interactif, où le système intègre progressivement les connaissances induites par les corrections de l’utilisateur. Nous avons étudié les caractéristiques de la langue arabe et nous avons conçu un système de reconnaissance pour cette langue. Dans un premier temps, nous avons adapté des méthodes de segmentation classiques, généralement utilisées pour les documents utilisant un alphabet latin. Nous avons constaté que les résultats obtenus par ces méthodes, peuvent être améliorés en intégrant des connaissances relatives à la classe de documents traitée. Nous préconisons pour cela l’intervention de l’utilisateur. L’idée est de transférer l’expertise de l’utilisateur vers le système de reconnaissance en convertissant ses corrections en connaissances. Ainsi, dans un deuxième temps, nous avons construit deux systèmes de reconnaissance pour traiter respectivement la reconnaissance physique ({PLANET}) et logique ({LUNET}) en utilisant un modèle évolutif qui s’adapte à toute nouvelle classe de documents. Le système {PLANET} utilise plusieurs modèles dédiés, chacun étant associé à une classe de documents donnés. La tâche de ces modèles est d'apprendre les caractéristiques propres à leur classe. Les modèles dédiés sont initialisés avec un modèle général qui est construit en vue d’avoir une connaissance générale de la superclasse de documents. Les systèmes {PLANET} et {LUNET} ont été évalués sur les classes de documents bien adaptés à la problématique : les journaux en langue arabe ({ANNAHAR}, {AL} {HAYAT} et {AL} {QUDS}). Après le traitement interactif de 10-15 pages de documents, le taux de reconnaissance passe de 96.729\% à 98.687\% ce qui correspond à une diminution du taux d’erreurs de 59.859\%. Quant à {LUNET}, le taux moyen de reconnaissance est de 94\% avec une diminution du taux d’erreurs de 63.436\%. Ainsi, nous estimons avoir démontré la pertinence d’utiliser des modèles évolutifs pour la reconnaissance de structures physiques et logiques de documents complexes. Ce type d’approche est particulièrement avantageux pour les applications de reconnaissance de taille moyenne ; c’est notamment le cas de la création de fonds de vérité qui est une opération fastidieuse et coûteuse. Grâce à {PLANET} / {LUNET} le processus de construction de tels fonds est simplifié. This thesis addresses the recognition of physical and logical structures of complex documents, rich in variability. More precisely, we studied the evolution of models within an interactive context where the system gradually integrates the knowledge induced by the corrections of the user. We studied the features of the Arabic language and we designed a recognition system for this language. In a first stage, we adapted traditional segmentation methods that are generally used for documents using a Latin alphabet. We noted that the results obtained by these methods, can be improved by integrating knowledge related to the treated class of documents. For that purpose we recommend the intervention of a user. The idea is to transfer the expertise from the user towards the recognition system by converting its corrections into knowledge. Thus, in the second stage, we built two systems for performing respectively the physical recognition ({PLANET}) and logic ({LUNET}) by using an evolutiv model which adapts to all new class of documents. The {PLANET} system uses several dedicated models; each one being associated a given class of documents. The task of these models is to learn the specific features of their class. The dedicated models are initialized with a general model, which is built in order to integrate general knowledge of a superclass of documents. The {PLANET} and {LUNET} systems have been evaluated on the classes of documents which are well adapted to the problematic: three classes of newspapers in Arabic language ({ANNAHAR}, {AL} {HAYAT} et {AL} {QUDS}). After the interactive treatment of 10- 15 pages, the recognition rate raised from 96.729\% to 98.687\% which corresponds to a reduction in the error rate of 59.859\%. As for {LUNET}, the average recognition rate is 94\% with a reduction in the error rate of 63.436\%. Thus, we estimate having shown the relevance of using evolutiv models for the recognition of the physical and logical structures, of complex documents. This type of approach is particularly advantageous for mid-sized applications; it is for instance the case of ground truth production, which is a tiresome and expensive operation. Thanks to {PLANET}/{LUNET} the process of building such ground truth is simplified.},
	author = {Hadjar, Karim},
	date = {2020-06-07}
}

@online{noauthor_fig2_nodate,
	title = {Fig.2 Modèle du neurone formel de Mac Culloch et Pitts (avec biais) Il...},
	url = {https://www.researchgate.net/figure/Modele-du-neurone-formel-de-Mac-Culloch-et-Pitts-avec-biais-Il-sagit-de-realiser_fig1_328996656},
	abstract = {Download scientific diagram {\textbar} Modèle du neurone formel de Mac Culloch et Pitts (avec biais) Il s'agit de réaliser l'apprentissage d'un réseau de neurones identificateur {RNI}, à partir des séquences des entrées de commande appliquées et des sorties mesurées. La figure 3 illustre le schéma de principe d'une identification directe [8]. from publication: Approche Adaptative d’une Commande Neuronale sans capteur d’un Moteur Asynchrone associée à un Observateur par Mode Glissant {\textbar} Cet article présente la commande neuronale d'une machine asynchrone sans capteur de vitesse, afin de remédier aux inconvénients de la commande avec capteur. Une commande neuronale pour la régulation de la vitesse associé à un observateur de type mode glissant a été donné pour... {\textbar} Engines, Observer and Vector Control {\textbar} {ResearchGate}, the professional network for scientists.},
	titleaddon = {{ResearchGate}},
	urldate = {2020-06-07},
	langid = {english},
	note = {Library Catalog: www.researchgate.net},
	file = {Snapshot:/home/paulin/Zotero/storage/X3GN5QQR/Modele-du-neurone-formel-de-Mac-Culloch-et-Pitts-avec-biais-Il-sagit-de-realiser_fig1_328996656.html:text/html}
}

@article{benharir_approche_2014,
	title = {Approche Adaptative d’une Commande Neuronale sans capteur d’un Moteur Asynchrone associée à un Observateur par Mode Glissant},
	abstract = {Cet article présente la commande neuronale d'une machine asynchrone sans capteur de vitesse, afin de remédier aux inconvénients de la commande avec capteur. Une commande neuronale pour la régulation de la vitesse associé à un observateur de type mode glissant a été donné pour estimer le flux rotorique et la vitesse mécanique de la {MAS}. Cet observateur est associé à une commande vectorielle classique. L'ensemble commande-observateur est testé sur les trajectoires de formes de consignes diverses. Les résultats obtenus par des essais effectués en simulation numérique, pour différents régimes de fonctionnement de l'ensemble onduleur, machine asynchrone et structure de commande, sont présentés ainsi qu'une analyse des performances de la stratégie de commande proposée. Mots clés-machine asynchrone, commande sans capteur, commande neuronale, control adaptative par mode glissant. Abstract-This paper presents the neural network control of a speed sensorless of asynchronous machine in order to overcome the disadvantages of control with sensor. The aim is to verify whether the contribution of artificial intelligence approaches brings significant improvements in terms in precision and robustness of speed estimation. A neural network control for regulating the speed associated with a sliding mode observer has been given to estimate the rotor flux and the mechanical speed of the induction motor. This observer is associated with a conventional vector control. Several tests were performed in simulation through significant operative conditions and have demonstrated the feasibility of the proposed approaches and validate the performance due to the contribution on intelligent techniques.},
	author = {Benharir, N and Zerikat, M and Chekroun, Soufyane and Mechernene, Abdelkader},
	date = {2014-01-01},
	file = {Full Text PDF:/home/paulin/Zotero/storage/E6BWWRZM/Benharir et al. - 2014 - Approche Adaptative d’une Commande Neuronale sans .pdf:application/pdf}
}

@article{misra_mish_2019-1,
	title = {Mish: A Self Regularized Non-Monotonic Neural Activation Function},
	url = {http://arxiv.org/abs/1908.08681},
	shorttitle = {Mish},
	abstract = {The concept of non-linearity in a Neural Network is introduced by an activation function which serves an integral role in the training and performance evaluation of the network. Over the years of theoretical research, many activation functions have been proposed, however, only a few are widely used in mostly all applications which include {ReLU} (Rectified Linear Unit), {TanH} (Tan Hyperbolic), Sigmoid, Leaky {ReLU} and Swish. In this work, a novel neural activation function called as Mish is proposed. The experiments show that Mish tends to work better than both {ReLU} and Swish along with other standard activation functions in many deep networks across challenging datasets. For instance, in Squeeze Excite Net- 18 for {CIFAR} 100 classification, the network with Mish had an increase in Top-1 test accuracy by 0.494\% and 1.671\% as compared to the same network with Swish and {ReLU} respectively. The similarity to Swish along with providing a boost in performance and its simplicity in implementation makes it easier for researchers and developers to use Mish in their Neural Network Models.},
	journaltitle = {{arXiv}:1908.08681 [cs, stat]},
	author = {Misra, Diganta},
	urldate = {2020-06-07},
	date = {2019-10-02},
	eprinttype = {arxiv},
	eprint = {1908.08681},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/paulin/Zotero/storage/R6DVWLAK/Misra - 2019 - Mish A Self Regularized Non-Monotonic Neural Acti.pdf:application/pdf;arXiv.org Snapshot:/home/paulin/Zotero/storage/WXQFLMSK/1908.html:text/html}
}

@book{minsky_perceptrons_2017,
	title = {Perceptrons: An introduction to computational geometry},
	isbn = {0-262-53477-0},
	publisher = {{MIT} press},
	author = {Minsky, Marvin and Papert, Seymour A},
	date = {2017}
}

@article{godfrey_continuum_2016,
	title = {A continuum among logarithmic, linear, and exponential functions, and its potential to improve generalization in neural networks},
	url = {http://arxiv.org/abs/1602.01321},
	abstract = {We present the soft exponential activation function for artificial neural networks that continuously interpolates between logarithmic, linear, and exponential functions. This activation function is simple, differentiable, and parameterized so that it can be trained as the rest of the network is trained. We hypothesize that soft exponential has the potential to improve neural network learning, as it can exactly calculate many natural operations that typical neural networks can only approximate, including addition, multiplication, inner product, distance, polynomials, and sinusoids.},
	journaltitle = {{arXiv}:1602.01321 [cs]},
	author = {Godfrey, Luke B. and Gashler, Michael S.},
	urldate = {2020-06-07},
	date = {2016-02-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1602.01321},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {Godfrey et Gashler - 2016 - A continuum among logarithmic, linear, and exponen.pdf:/home/paulin/Zotero/storage/C4SUHPNC/Godfrey et Gashler - 2016 - A continuum among logarithmic, linear, and exponen.pdf:application/pdf}
}

@online{verma_understanding_2020-1,
	title = {Understanding 1D and 3D Convolution Neural Network {\textbar} Keras},
	url = {https://towardsdatascience.com/understanding-1d-and-3d-convolution-neural-network-keras-9d8f76e29610},
	abstract = {When we say Convolution Neural Network ({CNN}), generally we refer to a 2 dimensional {CNN} which is used for image classification. But there…},
	titleaddon = {Medium},
	author = {Verma, Shiva},
	urldate = {2020-06-07},
	date = {2020-05-18},
	langid = {english},
	note = {Library Catalog: towardsdatascience.com},
	file = {Snapshot:/home/paulin/Zotero/storage/4ZH269GN/understanding-1d-and-3d-convolution-neural-network-keras-9d8f76e29610.html:text/html}
}

@online{jefkine_backpropagation_2016,
	title = {Backpropagation In Convolutional Neural Networks},
	url = {https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/},
	abstract = {Backpropagation in convolutional neural networks. A closer look at the concept of weights sharing in convolutional neural networks ({CNNs}) and an insight on how this affects the forward and backward propagation while computing the gradients during training.},
	titleaddon = {{DeepGrid}},
	author = {Jefkine},
	urldate = {2020-06-07},
	date = {2016-09-05},
	langid = {english},
	note = {Library Catalog: www.jefkine.com},
	file = {Snapshot:/home/paulin/Zotero/storage/QX6QTBMC/backpropagation-in-convolutional-neural-networks.html:text/html}
}

@article{rolnick_tackling_2019-1,
	title = {Tackling Climate Change with Machine Learning},
	url = {http://arxiv.org/abs/1906.05433},
	abstract = {Climate change is one of the greatest challenges facing humanity, and we, as machine learning experts, may wonder how we can help. Here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by machine learning, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the machine learning community to join the global effort against climate change.},
	journaltitle = {{arXiv}:1906.05433 [cs, stat]},
	author = {Rolnick, David and Donti, Priya L. and Kaack, Lynn H. and Kochanski, Kelly and Lacoste, Alexandre and Sankaran, Kris and Ross, Andrew Slavin and Milojevic-Dupont, Nikola and Jaques, Natasha and Waldman-Brown, Anna and Luccioni, Alexandra and Maharaj, Tegan and Sherwin, Evan D. and Mukkavilli, S. Karthik and Kording, Konrad P. and Gomes, Carla and Ng, Andrew Y. and Hassabis, Demis and Platt, John C. and Creutzig, Felix and Chayes, Jennifer and Bengio, Yoshua},
	urldate = {2020-06-10},
	date = {2019-11-05},
	eprinttype = {arxiv},
	eprint = {1906.05433},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	file = {arXiv.org Snapshot:/home/paulin/Zotero/storage/CS4SY2XA/1906.html:text/html;arXiv Fulltext PDF:/home/paulin/Zotero/storage/MYJXETD4/Rolnick et al. - 2019 - Tackling Climate Change with Machine Learning.pdf:application/pdf}
}