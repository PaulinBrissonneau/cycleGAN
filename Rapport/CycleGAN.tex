\chapter{Le cycleGAN}

\section{Présentation de la problématique}


Les cycleGAN, proposées par Zhu et al. \cite{zhu_unpaired_2018}, sont des architectures dérivées des GAN qui permettent de répondre à une problématique bien spécifique : le \textbf{transfert de style non appairé} \cite{gatys_image_2016}. Pour comprendre l’intérêt du cycleGAN, il faut bien comprendre le problème auquel il répond.
Le transfert de style consiste à transformer des données d'un \textit{style à un autre}. Le terme de \textit{style} est à prendre au sens large et les données que l'on manipule peuvent être de natures diverses. Il peut s'agir par exemple de transformer des images de pommes en images d'oranges, de transformer un paysage d'été en un paysage d'hiver, de transformer une musique classique en musique rock, ou encore de modifier les expressions faciales d'individus présents sur une image. Quelques exemples sont présentés sur la figure \ref{cycle_exemples}.

\begin{figure}[!h]
\centering
\includegraphics[width=200pt,valign=t]{"images/cycle/cycle_exemples"}
\caption{Exemples de transfert de style effectués par un cycleGAN. De haut en bas : transformation $pommes \leftrightarrow oranges$, transformation $paysages \, d'\acute{e} t \acute{e} \leftrightarrow paysages\,d'hiver$ et transformation $chevaux \leftrightarrow z\grave{e}bres$. Ces exemples sont tirés de l'article de Zhu et al. \cite{zhu_unpaired_2018}.}
\label{cycle_exemples}
\end{figure}


Le transfert de style peut se faire entre deux \textit{classes de styles} ou plus, mais nous allons ici nous concentrer sur le cas binaire, où l'on considère deux styles. La problématique est donc de transformer des images d'un style à l'autre, et ceci dans les deux sens.

Le transfert de style (à deux classes) repose sur deux banques de données que l'on notera ici A et B, et par extension nous parlerons du style A et du style B pour faire référence aux styles de ces banques de données. Suivant les données auxquelles nous avons accès, il existe deux cas différents :\\

\begin{itemize}
  \item Dans le cas où nous connaissons un appairage entre les images de A et de B, le problème est un \textbf{transfert de style appairé}. Le but est donc d'apprendre et de généraliser le transfert d'une donnée de A à une donnée de B à partir d'exemples de paires déjà existantes.\\
  \textit{Par exemple, si A représente des bâtiments de jour, et B représente des bâtiments de nuit, il est possible de prendre la même photo de jour et de nuit. Ces deux photos constituent une paire dont chaque élément est d'un style différent.}\\
  
  \item Dans le cas où chaque élément de A n'a pas de lien direct avec un élément de B en particulier, le problème est un \textbf{transfert de style non appairé}. Le but n'est plus d'apprendre et de généraliser le transfert d'une donnée de A à une donnée de B à partir d'exemples de paires déjà existantes, mais d'apprendre le transfert entre le style de A et le style de B, sans avoir d'exemple d'une telle transformation.\\
  \textit{Par exemple, si vous voulez transformer une image de votre chien en image de chat, vous ne pouvez pas obtenir une banque d'image de chiens déguisés en chats. Vous devez donc travailler avec d'une part des images de chiens (A), d'autre part des images de chats (B), sans pouvoir former de paires entre A et B.}\\
   La différence entre ces deux cas est illustrée par la figure \ref{paire}. 
\end{itemize}

\begin{figure}[!h]
\centering
\includegraphics[width=120pt,valign=t]{"images/cycle/paire"}
\hspace*{10mm}
\includegraphics[width=120pt,valign=t]{"images/cycle/pairepas"}
\caption{Les deux types de transferts de style. À gauche, le transfert de style appairé où chaque donnée est associée à son équivalent dans un autre style. À droite, le transfert de style non appairé où les images n'ont pas d'équivalent dans l'autre style. Ces images sont tirées de l'article de Zhu et al. \cite{zhu_unpaired_2018}.}
\label{paire}
\end{figure}

Ces deux types de transferts de style se traitent différemment. Pour le transfert de style appairé, une structure de GAN classique suffit puisque le discriminateur peut aisément comparer l'image générée avec l'image \textit{idéale}. Ce problème, que nous ne développerons pas ici, est traité et manière efficace par différents algorithmes, dont \textbf{Pix2Pix} proposé par Isola et al. en 2016 \cite{isola_image--image_2018}. Le transfert de style non appairé quant à lui ne permet pas la comparaison à l'image-cible puisqu'il n'existe pas de paires. \textbf{Il faut donc utiliser d'autres architectures, comme par exemple le cycleGAN.}


\section{Principe général du cycleGAN}

En vertu des explications présentées au paragraphe précédent, le problème se présente ainsi : nous avons une banque de données structurées A, et une banque de données structurées B, de même nature, dont les styles sont différents. Dans la suite, nous nous placerons dans le cas où ces données sont des images. Le but est de transformer les images de A pour leur donner le style des images de B, et inversement.

Par ailleurs, on remarque qu'en considérant la segmentation comme un style pour l'image, le cycleGAN peut aussi résoudre des problèmes de segmentation, même si il existe des algorithmes beaucoup plus efficaces spécialisés sur ce problème.\\

Le cycleGAN repose sur deux GAN, tête-bêche, l'un permettant de passer du style A au style B, l'autre du style B au style A. Plus précisément, il y a deux générateurs, un générateur qui prend des images de la banque A et doit générer des images du style de B (noté G), l'autre qui prend des images de la banque B et doit générer des images du style de A (noté F). Il y a aussi deux discriminateurs, notés $D_A$ et $D_B$, qui respectivement discriminent les images du style A et celles du style B. L'architecture est présentée par la figure \ref{cycleDouble}.

\begin{figure}[!h]
\centering
\includegraphics[width=250pt]{"images/cycle/cycleDouble"}
\caption{Structure globale du cycleGAN. Le générateur G créé des images du style de B à partir d'images du style de A. Le générateur F créé des images du style de A à partir d'images du style de B. Chaque banque d'images, associée à un style, possède son discriminateur.}
\label{cycleDouble}
\end{figure}

Comme on l'a entrevu dans le paragraphe précédent, une difficulté est que les données ne sont pas appairées, la fonction de coût ne peut donc pas venir de la comparaison directe de l'image générée à l'image souhaitée. Pour pallier ce manque, deux fonctions de coûts principales et indépendantes sont utilisées.\\

La première est celle d'un GAN classique : pour une transformation $ A \rightarrow B $ (resp. $ B \rightarrow A $), le discriminateur $ D_B $ (resp. $ D_A $) prédit si l'image est une image qui appartient réellement à la banque B (resp. A). Le coût associé à chacun des deux GAN ainsi définis est appelé \textit{Adversarial Loss} ou \textit{GAN Loss}. La figure \ref{cycle_ganGF} montre la décomposition du cycleGAN en deux GAN. \\

\begin{figure}[!h]
\centering
\includegraphics[width=150pt]{"images/cycle/cycle_ganG"}
\hspace*{10mm}
\includegraphics[width=150pt]{"images/cycle/cycle_ganF"}
\caption{Décomposition du cycleGAN en deux GAN distincts.}
\label{cycle_ganGF}
\end{figure}

Comme on peut s'y attendre, cela ne suffit pas. En effet, si l'on considère seulement ce coût, comment peut-on s'assurer que l'image obtenue a encore un lien avec l'image de départ ? Pour garantir cela, il faut s'assurer de pouvoir reconstruire l'image de départ après lui avoir fait subir  la transformation $ A \rightarrow B $ suivie de $ B \rightarrow A $. En d'autres termes, cela revient à ajouter des conditions sur les générateurs G et F telles que :

\begin{equation}
\begin{split}
\forall a \in A, F(G(a)) \approx a \\
\forall b \in B, G(F(b)) \approx b
\end{split}
\end{equation}

Le coût qui en découle (et qui sera détaillé dans la suite), est appelé \textit{Cycle Consistency Loss}. Les deux égalités ci-dessus consistent en réalité à parcourir le cycle respectivement en avant et en arrière, elles sont appelées respectivement \textit{backward cycle consistency} et \textit{forward cycle consistency} et sont utilisées depuis longtemps dans le suivi d'objets \cite{kalal_forward-backward_2010}. Ceci est décrit de manière schématique par la figure \ref{cycleForBack}.\\

\begin{figure}[!h]
\centering
\includegraphics[width=250pt]{"images/cycle/cycleBack"}

\vspace{6mm}

\includegraphics[width=250pt]{"images/cycle/cycleFor"}
\caption{Décomposition du cycleGAN en deux GAN distincts et ajout des contraintes de consistance cyclique. En haut, la consistance cyclique arrière (\textit{backward cycle consistency}). En bas, la consistance cyclique avant (\textit{forward cycle consistency}).}
\label{cycleForBack}
\end{figure}

Pour résumer le fonctionnement global du cycleGAN : le générateur G (qui assure la transformation $ A \rightarrow B $) est optimisé pour tromper le discriminateur $ D_B $ comme dans un GAN classique, mais aussi pour que, à F fixé, $ F \circ G = \mathbb{1} $. Symétriquement, il en est de même pour le générateur F (qui assure la transformation $ B \rightarrow A $). Les discriminateurs, quant à eux, sont mis à jour selon la même fonction de coût qu'un discriminateur de GAN classique. Les fonctions de coûts utilisées sont détaillées dans la partie suivante.


\section{Les fonctions de coûts}

\subsubsection{Coût adversaire : \textit{GAN Loss}}

Comme précisé dans la partie précédente, le coût associé au caractère adversaire de l'apprentissage est celui d'un GAN classique \cite{goodfellow_nips_2016}. Avec les mêmes notations que dans le paragraphe précédent, en considérant le générateur G et son discriminateur associé $D_B$, on a :
$$\begin{aligned}
\mathcal{L}_{\mathrm{GAN}}\left(G, D_{B}, A, B\right) &=\mathbb{E}_{b \sim p_{\mathrm{data}}(b)}\left[\log D_{B}(b)\right] +\mathbb{E}_{a \sim p_{\text {data }}(a)}\left[\log \left(1-D_{B}(G(a))\right]\right.
\end{aligned}$$

Comme dans le cas d'un GAN classique, le générateur tend à minimiser ce coût et le discriminateur tend à le maximiser.

Pour l'autre GAN, c'est-à-dire le générateur F et son discriminateur $D_A$, on a de même : $$\begin{aligned}
\mathcal{L}_{\mathrm{GAN}}\left(F, D_{A}, B, A\right) &=\mathbb{E}_{a \sim p_{\mathrm{data}}(a)}\left[\log D_{A}(a)\right] +\mathbb{E}_{b \sim p_{\text {data }}(b)}\left[\log \left(1-D_{A}(G(b))\right]\right.
\end{aligned}$$

\subsubsection{Coût du cycle : \textit{Cycle Consistency Loss}}

Conformément aux explications données dans le paragraphe précédent, on cherche une fonction de coût qui assure que : $ F \circ G = \mathbb{1} $ et $ G \circ F = \mathbb{1} $. Il est important de noter que l'on veut un coût qui n'intervienne pas à une hauteur sémantique. On considère donc deux comparaisons pixel à pixel, une pour la \textit{backward cycle consistency} et une pour la \textit{forward cycle consistency}, que l'on somme. La fonction de coût qui en découle est donc :

$$\begin{aligned}
\mathcal{L}_{\mathrm{cyc}}(G, F) &=\mathbb{E}_{a \sim p_{\text {data }}(a)}\left[\|F(G(a))-a\|_{1}\right] +\mathbb{E}_{b \sim p_{\text {data }}(b)}\left[\|G(F(b))-b\|_{1}\right]
\end{aligned}$$

\subsubsection{Fonction de coût globale}

Les deux fonctions de coûts adversaires jouent des rôles symétriques donc elles ont la même importance dans la forme de la fonction de coût globale. Cependant, rien ne laisse penser que l'importance de la fonction de coût du cycle leur est aussi équivalente. Il est donc nécessaire d'introduire un $\lambda \in \mathbb{R}$ tel que :

$$\begin{aligned}
\mathcal{L}_{\text{total}} &=\mathcal{L}_{\mathrm{GAN}}\left(G, D_{B}, A, B\right) +\mathcal{L}_{\mathrm{GAN}}\left(F, D_{A}, B, A\right) +\lambda \cdot \mathcal{L}_{\mathrm{cyc}}(G, F)
\end{aligned}$$

$\lambda$ est un hyper-paramètre. D'après Zhu et al. \cite{zhu_unpaired_2018}, $\lambda \approx 10$ donne les meilleurs résultats.

\subsubsection{Préservation de la couleur}

Pour certaines applications particulières, notamment pour le traitement de paysages, il est nécessaire de rajouter un autre terme à la fonction de coût. En effet, comme on l'observe sur la figure \ref{Lident} les couleurs globales des photos en entrée ne sont pas inchangées en sortie. Les images sont par exemple bleuies ou  jaunies. Dans l'article de Zhu et al. \cite{zhu_unpaired_2018}, l'équipe propose de contraindre encore plus l'espace dans lequel évoluent les générateurs du cycleGAN, par une technique introduite par Taigman et al. \cite{taigman_unsupervised_2016}. L'idée consiste à ajouter un coût demi-cyclique qui tend à ce que $ F \approx \mathbb{1} $ et $ G \approx \mathbb{1} $. On rajoute donc un coût $\mathcal{L}_{\text {identity }}$ défini comme :

$$\mathcal{L}_{\text {identity }}(G, F)=\mathbb{E}_{b \sim p_{\text {data}}(b)}\left[\|G(b)-b\|_{1}\right]+ \mathbb{E}_{a \sim p_{\text {data}}(a)}\left[\|F(a)-a\|_{1}\right]$$

On comprend bien que c'est une limitation très forte, qui ne convient qu'à certains problèmes pour lesquels les images de sortie sont très proches des images d'entrée et pour lesquels la couleur ne doit pas beaucoup changer. Sous ces conditions, il se trouve que cette méthode conserve efficacement la composition des couleurs, comme peut l'attester la figure \ref{Lident}. 

\begin{figure}[!h]
\centering
\includegraphics[width=200pt]{"images/cycle/Lident"}
\caption{Mise en évidence de la dégradation de la composition des couleurs, par Zhu et al. \cite{zhu_unpaired_2018}. À droite, l'effet de la fonction de coût $\mathcal{L}_{\text {identity }}$ qui améliore la cohérence des couleurs.}
\label{Lident}
\end{figure}


\section{Les métriques d'évaluations}

Comme dans le cas d'un GAN classique, évaluer la qualité de la sortie d'un cycleGAN n'est pas une chose facile. En effet, nous n'avons pas de métrique simple et universelle qui permettrait de juger de la crédibilité ou du réalisme d'une image. Pour tenter d'évaluer au mieux la qualité d'un cycleGAN il existe plusieurs solutions.\\

La première, sans grande surprise, c'est de faire une étude de réalisme basée sur une enquête auprès de personnes chargées de noter la qualité des images fournies, c'est ce que l'on appelle des études de perceptions (\textit{perceptual studies}). On comprend vite que ce n'est une très bonne solution : ces études restent subjectives, elles ne sont pas toujours reproductibles, et elles coûtent cher. Comme pour les GAN, on ne peut pas donc pas s'en servir pour poser une métrique universelle pour comparer différents algorithmes.\\

Pour quelques problèmes particuliers, on peut trouver des métriques convenables. C'est le cas par exemple si l'on considère un problème de segmentation et si les données sont accompagnées de leurs segmentations réelles, appelées aussi \textit{ground truth}. Dans ce cas particulier, évaluer le cycleGAN revient simplement à évaluer le résultat de la segmentation par rapport au \textit{ground truth}. Il existe plusieurs métriques classiques pour évaluer les algorithmes de segmentation comme par exemple la précision pixel à pixel ou la précision classes à classes, mais la métrique la plus courante pour cela est l'indice de Jaccard (ou \textit{IoU : Intersection over Union}). Cette métrique consiste à calculer, pour chaque classe de la segmentation, l'intersection de la zone prédite par l'algorithme avec la zone réelle, avant de normaliser par l'union des deux zones. C'est une métrique classique utilisée en segmentation, elle est définie ci-dessous.

$$ \textit{Indice de Jaccard : }J(A,B) = \frac{ \mid A \cap B \mid }{ \mid A \cup B \mid } $$

Cependant, dans le cas général, le problème n'est pas un problème de segmentation, mais un problème de génération d'images réalistes suivant un style, donc la métrique précédente n'est pas utilisable. Il en existe d'autres métriques, par exemple le \textbf{score FCN}. Le score FCN consiste à évaluer l'interprétabilité des images de sortie par un algorithme classique de segmentation sémantique (ici le FCN, pour \textit{Fully Convolutional Networks for Semantic Segmentation} \cite{long_fully_2014}). Sur une image générée par le cycleGAN, le FCN prédit une carte de segmentation. Cette carte de segmentation est ensuite comparée à l’image d’entrée avec les métriques classiques que l'on a évoquées au-dessus, en particulier l'indice de Jaccard.\\
Notons que le score FCN ne permet pas de vérifier que le style de l'image est correct, mais seulement d'évaluer grossièrement le caractère réaliste de l'image, à travers l'interprétabilité de l'image par un autre algorithme. En somme, il n'existe aucune métrique idéale pour évaluer les cycleGAN.


\section{Implémentation et résultats}

\subsection{Détails d'implémentation}

Notre implémentation, comme pour les autres algorithmes, utilise TensorFlow 2.0. Nous avons globalement respecté la structure des GAN préconisée dans l'article de Zhu et al. \cite{zhu_unpaired_2018}, qui a été proposée par Johnson  et  al. \cite{johnson_perceptual_2016} mais nous avons adapté l'architecture à chacune de nos banques de données. L'architecture de base, comme décrite dans l'article est la suivante : \\

\textbf{Pour le discriminateur}, nous avons utilisé un PatchGAN \cite{isola_image--image_2018-3}. Avec les notations utilisées par TensorFlow :


\[ \begin{array}{lcr}
Conv2D(64, (4,4), strides=(2,2)) \\
LeakyReLU(alpha=0.2) \\

Conv2D(128, (4,4), strides=(2,2)) \\
InstanceNormalization\\
LeakyReLU(alpha=0.2)\\

Conv2D(256, (4,4), strides=(2,2)) \\
InstanceNormalization\\
LeakyReLU(alpha=0.2)\\

Conv2D(512, (4,4), strides=(2,2)) \\
InstanceNormalization\\
LeakyReLU(alpha=0.2)\\

Conv2D(512, (4,4), strides=(2,2)) \\
InstanceNormalization\\
LeakyReLU(alpha=0.2)\\

Conv2D(1, (4,4))\end{array}\]

À noter que toutes les convolutions ont un \textit{padding} défini sur \textit{same}. La couche \textit{Instance Normalization} fait référence à la normalisation présentée par Ulyanov et al. \cite{ulyanov_instance_2017}. \\

\textbf{Pour le générateur}, toujours comme dans l'article de Zhu et al. \cite{zhu_unpaired_2018}, nous avons utilisé un réseau résiduel (proposé par He et al. \cite{he_deep_2015}). Avec les notations utilisées par TensorFlow :\\

Soit le bloc résiduel ($ResBlock$) de paramètre $n_{filters}$ défini par :


\[ \begin{array}{lcr}
Conv2D(n_{filters}, (3,3)) \\
InstanceNormalization \\
Activation(relu) \\
Conv2D(n_{filters}, (3,3))\\
g = InstanceNormalization\\
Concatenate()([g, input layer]) \end{array}\]

Le générateur complet s'écrit :

\[ \begin{array}{lcr}
Conv2D(64, (7,7)) \\
Activation(relu) \\

Conv2D(128, (3,3)) \\
InstanceNormalization\\
Activation(relu)\\

Conv2D(256, (3,3)) \\
InstanceNormalization\\
Activation(relu)\\\\

N \times [ResBlock(n_{filters})]\\\\

Conv2DTranspose(128, (3,3)) \\
InstanceNormalization\\
Activation(relu)\\

Conv2DTranspose(64, (3,3)) \\
InstanceNormalization\\
Activation(relu)\\

Conv2D(3, (7,7)) \\
InstanceNormalization\\

Activation(tanh) \end{array}\]

À noter que toutes les convolutions ont un \textit{padding} défini sur \textit{same} et des \textit{strides} de $(2, 2)$.

Notons que $n_{filters}$ et $N$ sont des hyper-paramètres. Leur valeur dépend de la taille des images et de la puissance de calcul disponible. Sur les conseils de Zhu et al., nous utilisons $n_{filters} = 256$ et $N \in [5, 10]$. Dans notre implémentation, tous les hyper-paramètres du modèle sont facilement modifiables depuis un unique fichier. \\


Pour les paramètres d'apprentissage, nous avons aussi suivi les conseils de l'article puis nous avons adapté les valeurs à chaque banque d'images en testant différentes valeurs possibles. Nous avons, de manière générale, les valeurs nominales suivantes : \\

\begin{itemize}
  \item Nombre de passes : 150
  \item Taille des batch : 1
  \item Optimiseur : Adam
  \item $\alpha_{Adam}$ : 0.0002 puis linéairement décroissant à partir de la passe 100
  \item $\beta1_{Adam}$ : 0.5\\
\end{itemize}

Comme pour les GAN, la stabilité du modèle peut être améliorée en entraînant le discriminateur sur un historique des images générées. Cette technique a été proposée par Shrivastava  et  al. \cite{shrivastava_learning_2016} et reprise par Zhu et al. pour les cyclesGAN. Nous l'avons aussi implémentée. Il en découle un nouvel hyper-paramètre : la taille du \textit{buffer} contenant l'historique des images générées. Nous prenons, comme proposé dans l'article de Zhu et al., $buffer_{max} = 50$.

\subsection{Quelques résultats}

Quelques exemples de nos résultats sont présentés sur les figures \ref{exemple_celebA} à \ref{exemple_zebres}

\begin{figure}[!h]
\centering
\includegraphics[width=300pt]{"images/cycle/cycle"}
\caption{Exemples de sorties de notre cycleGAN sur la banque d'images CelebA. La première ligne correspond aux images de la banque, la deuxième ligne correspond à la sortie du générateur. À gauche, il s'agit de la transformation \textit{portrait sans sourire} vers \textit{portrait avec sourire}. À droite, il s'agit de la transformation inverse.}
\label{exemple_celebA}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width=150pt]{"images/cycle/cycleRes2"}
\caption{Exemples de sorties de notre cycleGAN sur la banque d'images $pommes \leftrightarrow oranges$. À gauche, les images d'orange de la banque. À droite, les mêmes images dans le style des pommes en sortie de cycleGAN.}
\label{exemple_orange}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width=150pt]{"images/cycle/cycleRes3"}
\caption{Exemple de sortie de notre cycleGAN sur la banque d'image $chevaux \leftrightarrow z\grave{e}bres$. À gauche, une image de chevaux de la banque. À droite, la même image dans le style des zèbres en sortie de cycleGAN.}
\label{exemple_zebres}
\end{figure}

\section{Utilisation du Mésocentre Moulon}

Comme montré dans la partie  concernant l'implémentation, les réseaux de neurones utilisés pour les discriminateurs et les générateurs des GAN sont de très grandes tailles. Leurs nombres de paramètres entraînables se comptent vite en millions puis dizaines de millions. Or les cycleGAN sont en première approximation deux GAN tête-bêche, on comprend donc bien qu'ils sont des algorithmes très lourds et demandent une puissance de calcul colossale. Pour donner un ordre d'idée, la mémoire vive d'un ordinateur classique ne permet pas l’entraînement de tels réseaux, sans même prendre de compte le temps de calcul que cela prendrait. Pour entraîner des réseaux de cette taille, il est nécessaire d'avoir accès à une puissance de calcul annexe. Dans notre cas, c'est le \textbf{Mésocentre Moulon} (le mésocentre de CentraleSupélec et de l'ENS Paris-Saclay)  qui nous permet d’exécuter nos algorithmes, ce qui en fait \textbf{un outil indispensable au déroulé du projet}.\\
L'utilisation du mésocentre requiert une petite attention particulière, puisqu'elle nécessite d'utiliser des scripts PBS. Une documentation de qualité et une réunion préalable ont permis de grandement faciliter la migration de nos algorithmes vers le cluster de GPU. Tous nos scripts sont maintenant exécutables à la fois sur nos CPU personnels et le cluster de GPU Fusion du mésocentre.\\
Cependant, notre implémentation du cycleGAN n'est pas parfaite. En effet, la durée d’exécution de chaque programme sur les GPU Fusion est limitée à 24h. C'est un temps largement suffisant pour la plupart des programmes que nous avons à exécuter, mais il s'avère que sur certaines bases de données d'images très larges, notre implémentation du cycleGAN requiert plus de 24h (d'après nos estimations, nous pourrions atteindre les 150 passes voulues en à peu près 30h). Pour l'instant, nous n'arrivons pas à enregistrer nos modèles - ainsi que toutes les données nécessaires à la reprise de l'apprentissage - en cours d’entraînement. Ceci empêche la reprise de l'apprentissage dans de bonnes conditions, et fausse nos résultats. C'est pourquoi pour l'instant nos résultats sont convenables que sur des images de petites dimensions qui ne demandent qu'un temps de calcul inférieur à 24h. Ce problème est en cours de résolution, et nous pourrons prochainement utiliser les clusters du mésocentre Moulon pour des entraînements de plusieurs journées.



\section{Conclusion}

Les résultats que nous obtenons pour l'instant sont corrects mais restent mitigés, notament à cause du détail d'implémentation expliqué dans la partie précédente. Cependant, étant donné que sur les premières passes nous obtenons des résultats cohérent avec les implémentations de référence, nous sommes plutôt confiants quant à la qualité de notre implémentation. Son caractère modulaire et sa paramétrabilité très facile permettent de l'adapter à beaucoup de banques d'images différentes très rapidement. C'est ce qui nous permet d'utiliser notre script pour des problèmes non abordés dans l'article de Zhu et al. \cite{zhu_unpaired_2018}, comme les sourires sur CelebA. De plus, on peut l'utiliser de manière équivalente sur nos CPU personnels et sur le cluster de GPU Fusion.\\

Une fois les derniers détails d'implémentations réglés (dans peu de temps) nous pourrons utiliser notre programme pour étudier un nouveau problème, que nous devons définir.